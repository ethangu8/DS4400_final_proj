{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924300b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f5a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data as csv files from Kaggle, values are already normalized and split into test and training data\n",
    "test_x = pd.read_csv('bankruptcy_Test_X.csv')\n",
    "train = pd.read_csv('bankruptcy_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d805bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.007954</td>\n",
       "      <td>-0.007140</td>\n",
       "      <td>-0.003544</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>-0.004687</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>-0.006963</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>-0.009751</td>\n",
       "      <td>-0.002655</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.014331</td>\n",
       "      <td>-0.006864</td>\n",
       "      <td>0.020750</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>0.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.396405</td>\n",
       "      <td>1.409440</td>\n",
       "      <td>1.015494</td>\n",
       "      <td>0.908020</td>\n",
       "      <td>1.394750</td>\n",
       "      <td>1.286713</td>\n",
       "      <td>1.412509</td>\n",
       "      <td>1.064426</td>\n",
       "      <td>1.171199</td>\n",
       "      <td>1.407349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>0.718682</td>\n",
       "      <td>0.026124</td>\n",
       "      <td>0.467139</td>\n",
       "      <td>1.252571</td>\n",
       "      <td>1.171160</td>\n",
       "      <td>0.277288</td>\n",
       "      <td>1.115182</td>\n",
       "      <td>0.930875</td>\n",
       "      <td>0.141032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-9.474787</td>\n",
       "      <td>-140.604555</td>\n",
       "      <td>-25.597146</td>\n",
       "      <td>-0.381641</td>\n",
       "      <td>-138.720013</td>\n",
       "      <td>-26.249562</td>\n",
       "      <td>-141.176615</td>\n",
       "      <td>-0.966015</td>\n",
       "      <td>-1.294340</td>\n",
       "      <td>-2.528495</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.657288</td>\n",
       "      <td>-42.381245</td>\n",
       "      <td>-0.037484</td>\n",
       "      <td>-3.218490</td>\n",
       "      <td>-0.048598</td>\n",
       "      <td>-0.412525</td>\n",
       "      <td>-0.022204</td>\n",
       "      <td>-0.743297</td>\n",
       "      <td>-0.050969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.079776</td>\n",
       "      <td>-0.055604</td>\n",
       "      <td>-0.516971</td>\n",
       "      <td>-0.192306</td>\n",
       "      <td>0.006798</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>-0.008018</td>\n",
       "      <td>-0.445667</td>\n",
       "      <td>-0.388184</td>\n",
       "      <td>-0.068584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009315</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>-0.010766</td>\n",
       "      <td>-0.022740</td>\n",
       "      <td>-0.043418</td>\n",
       "      <td>-0.221237</td>\n",
       "      <td>-0.016847</td>\n",
       "      <td>-0.371384</td>\n",
       "      <td>-0.041603</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.019204</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>-0.117972</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>-0.016047</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>-0.260150</td>\n",
       "      <td>-0.307842</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009838</td>\n",
       "      <td>0.010013</td>\n",
       "      <td>-0.009924</td>\n",
       "      <td>-0.020438</td>\n",
       "      <td>-0.039928</td>\n",
       "      <td>-0.139653</td>\n",
       "      <td>-0.013821</td>\n",
       "      <td>-0.180311</td>\n",
       "      <td>-0.034319</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.081218</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>0.585722</td>\n",
       "      <td>0.022705</td>\n",
       "      <td>0.007753</td>\n",
       "      <td>0.016026</td>\n",
       "      <td>0.024222</td>\n",
       "      <td>0.098874</td>\n",
       "      <td>0.156638</td>\n",
       "      <td>0.070036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010693</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>-0.009416</td>\n",
       "      <td>-0.014401</td>\n",
       "      <td>-0.031820</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.009476</td>\n",
       "      <td>0.133244</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>137.557872</td>\n",
       "      <td>2.004925</td>\n",
       "      <td>3.324327</td>\n",
       "      <td>60.825460</td>\n",
       "      <td>11.556238</td>\n",
       "      <td>121.354736</td>\n",
       "      <td>0.623095</td>\n",
       "      <td>47.504463</td>\n",
       "      <td>62.936396</td>\n",
       "      <td>140.123299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036727</td>\n",
       "      <td>37.405312</td>\n",
       "      <td>2.551674</td>\n",
       "      <td>27.969785</td>\n",
       "      <td>117.341069</td>\n",
       "      <td>59.139158</td>\n",
       "      <td>23.970250</td>\n",
       "      <td>62.527240</td>\n",
       "      <td>90.774695</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Attr1         Attr2         Attr3         Attr4         Attr5  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.007954     -0.007140     -0.003544      0.005005     -0.004687   \n",
       "std        1.396405      1.409440      1.015494      0.908020      1.394750   \n",
       "min       -9.474787   -140.604555    -25.597146     -0.381641   -138.720013   \n",
       "25%       -0.079776     -0.055604     -0.516971     -0.192306      0.006798   \n",
       "50%       -0.019204      0.000246      0.003186     -0.117972      0.007302   \n",
       "75%        0.081218      0.056957      0.585722      0.022705      0.007753   \n",
       "max      137.557872      2.004925      3.324327     60.825460     11.556238   \n",
       "\n",
       "              Attr6         Attr7         Attr8         Attr9        Attr10  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.001455     -0.006963      0.009053      0.006763      0.007456   \n",
       "std        1.286713      1.412509      1.064426      1.171199      1.407349   \n",
       "min      -26.249562   -141.176615     -0.966015     -1.294340     -2.528495   \n",
       "25%       -0.016047     -0.008018     -0.445667     -0.388184     -0.068584   \n",
       "50%       -0.016047      0.004280     -0.260150     -0.307842      0.000253   \n",
       "75%        0.016026      0.024222      0.098874      0.156638      0.070036   \n",
       "max      121.354736      0.623095     47.504463     62.936396    140.123299   \n",
       "\n",
       "       ...        Attr56        Attr57        Attr58        Attr59  \\\n",
       "count  ...  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean   ...      0.009804      0.005208     -0.009751     -0.002655   \n",
       "std    ...      0.017370      0.718682      0.026124      0.467139   \n",
       "min    ...     -1.657288    -42.381245     -0.037484     -3.218490   \n",
       "25%    ...      0.009315      0.003199     -0.010766     -0.022740   \n",
       "50%    ...      0.009838      0.010013     -0.009924     -0.020438   \n",
       "75%    ...      0.010693      0.019900     -0.009416     -0.014401   \n",
       "max    ...      0.036727     37.405312      2.551674     27.969785   \n",
       "\n",
       "             Attr60        Attr61        Attr62        Attr63        Attr64  \\\n",
       "count  10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
       "mean       0.004548      0.014331     -0.006864      0.020750     -0.003984   \n",
       "std        1.252571      1.171160      0.277288      1.115182      0.930875   \n",
       "min       -0.048598     -0.412525     -0.022204     -0.743297     -0.050969   \n",
       "25%       -0.043418     -0.221237     -0.016847     -0.371384     -0.041603   \n",
       "50%       -0.039928     -0.139653     -0.013821     -0.180311     -0.034319   \n",
       "75%       -0.031820     -0.002210     -0.009476      0.133244     -0.020507   \n",
       "max      117.341069     59.139158     23.970250     62.527240     90.774695   \n",
       "\n",
       "              class  \n",
       "count  10000.000000  \n",
       "mean       0.020300  \n",
       "std        0.141032  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cc1e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another data source that unfortunately doesn't use the same columns so I don't think we can use it\n",
    "other = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5b4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics function we can use to evaluate our models\n",
    "def metrics(y, y_pred):\n",
    "    '''\n",
    "    Parameters:\n",
    "    y, y_pred (Pandas Series): series representing actual labels(y) and predicted outcomes(y_pred)\n",
    "    \n",
    "    Returns:\n",
    "    model accuracy, sensitivity, specificity, precision, f1-score\n",
    "    '''\n",
    "    # Create a confusion matrix with our two series\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "     # Accuracy Calcs\n",
    "    accuracy = (TN + TP)/(TN + FP + FN + TP) \n",
    "    \n",
    "    # Sensitivity Calcs\n",
    "    # TP (1,1)/ (TP (1,1) + FN (1,0)) <- wherever y actual = 1\n",
    "    sensitivity = TP/(TP + FN)\n",
    "    \n",
    "    # Specificity Calc\n",
    "    specificity = TN/(TN + FP)\n",
    "    \n",
    "    # Precision Calc\n",
    "    precision = TP/(TP + FP)\n",
    "    \n",
    "    # F1 Score Calc\n",
    "    f1_score = 2 * ((precision * sensitivity) / (precision + sensitivity))\n",
    "    \n",
    "    #print(f'Accuracy: {accuracy} \\nSensitivity: {sensitivity}\\nSpecificity: {specificity}\\nPrecision: {precision}\\nf1 score: {f1_score}')\n",
    "    return {'Accuracy': round(accuracy, 4), 'Sensitivity': round(sensitivity, 4), \n",
    "            'Specificity': round(specificity, 4), 'Precision': round(precision, 4), 'f1 score': round(f1_score, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14d9905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 9797, 1: 203})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the amount of companies that went bankrupt in our data\n",
    "# 0 = did not bankrupt, 1 = bankrupt\n",
    "Counter(train['class'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb42c3",
   "metadata": {},
   "source": [
    "## Naive Bayes (not going to use)\n",
    "Despite our project proposal suggesting Naive Bayes as a way to classify our data, we decided against it. Reason being, we wanted to implement as many of our attributes in our data as possible, and in doing so, it would be more difficult to calculate probabilities given the sheer amount of attributes that our data contains. Theoretically, we could split our data into smaller subsets of attributes, but we didn't think that it would provide an accurate enough classification given that our attribute list is over 60 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18950bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8671d5d",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e96c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethan's knn function from HW 3\n",
    "from heapq import nlargest # library to get the keys of the largest values in a dict\n",
    "def knn(vector_list, speakers, k, function):\n",
    "    '''\n",
    "    given a dataframe, k neighbors, and a distance metric function\n",
    "    compare each word vector against every other word vector\n",
    "    find the k vectors with the highest distance value (in the case of cossim, for euclidean it would need to be smallest)\n",
    "    find the speakers of those k vectors\n",
    "    '''\n",
    "    guesses = list()\n",
    "    idx_vector_dict = dict(zip(list(range(0, 1000)), vector_list))\n",
    "    # Loop through our word vectors\n",
    "    for idx, vector in idx_vector_dict.items():\n",
    "        distance_dict = {}\n",
    "        others_dict = {v: idx_vector_dict[v] for v in idx_vector_dict.keys() - {idx}} # so we don't compare against itself\n",
    "        for i, j in others_dict.items(): # loop through all other word vectors\n",
    "            distance = function(vector, j) # can sub in another distance function is desired\n",
    "            distance_dict[i] = distance\n",
    "\n",
    "        # get the largest differences, which is the most similar for cosine similarity\n",
    "        k_largest = nlargest(k, distance_dict, key=distance_dict.get) \n",
    "        s_list = [speakers[i] for i in k_largest] # find the speaker for each of the nearest neighbors\n",
    "        guess = max(set(s_list), key=s_list.count) # find out which speaker is represented more\n",
    "        guesses.append(guess) # append our guess to the list\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4c769",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e1a42",
   "metadata": {},
   "source": [
    "Another approach, inspired by https://www.kaggle.com/code/karthik7395/binary-classification-using-neural-networks/notebook\n",
    "\n",
    "make sure to run these if not already installed:\n",
    "- pip install keras\n",
    "- pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a9466af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "316b902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units=100\n",
    "learning_rate=0.01\n",
    "hidden_layer_act='tanh'\n",
    "output_layer_act='sigmoid'\n",
    "no_epochs=100\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05159922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,385\n",
      "Trainable params: 8,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(64, input_shape=(64,), activation=hidden_layer_act))\n",
    "model.add(Dense(64, activation=hidden_layer_act))\n",
    "model.add(Dense(1, activation=output_layer_act))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39b38e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd=optimizers.SGD(learning_rate=learning_rate)\n",
    "model.compile(loss='binary_crossentropy',optimizer=sgd, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b74b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.6912 - acc: 0.6204 - 312ms/epoch - 312ms/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6834 - acc: 0.6341 - 16ms/epoch - 16ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.6757 - acc: 0.6474 - 15ms/epoch - 15ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.6682 - acc: 0.6599 - 15ms/epoch - 15ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.6608 - acc: 0.6743 - 16ms/epoch - 16ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.6535 - acc: 0.6842 - 16ms/epoch - 16ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.6463 - acc: 0.6944 - 15ms/epoch - 15ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.6392 - acc: 0.7033 - 16ms/epoch - 16ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.6323 - acc: 0.7131 - 16ms/epoch - 16ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.6255 - acc: 0.7228 - 15ms/epoch - 15ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.6188 - acc: 0.7326 - 15ms/epoch - 15ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.6122 - acc: 0.7431 - 14ms/epoch - 14ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.6057 - acc: 0.7547 - 14ms/epoch - 14ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.5993 - acc: 0.7617 - 15ms/epoch - 15ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.5930 - acc: 0.7699 - 15ms/epoch - 15ms/step\n",
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.5868 - acc: 0.7780 - 15ms/epoch - 15ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.5807 - acc: 0.7871 - 15ms/epoch - 15ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.5747 - acc: 0.7956 - 15ms/epoch - 15ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.5688 - acc: 0.8039 - 14ms/epoch - 14ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.5630 - acc: 0.8114 - 16ms/epoch - 16ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.5573 - acc: 0.8180 - 14ms/epoch - 14ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.5517 - acc: 0.8245 - 15ms/epoch - 15ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.5461 - acc: 0.8311 - 15ms/epoch - 15ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.5407 - acc: 0.8366 - 15ms/epoch - 15ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.5353 - acc: 0.8424 - 15ms/epoch - 15ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.5300 - acc: 0.8489 - 15ms/epoch - 15ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.5248 - acc: 0.8541 - 14ms/epoch - 14ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.5196 - acc: 0.8589 - 16ms/epoch - 16ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.5146 - acc: 0.8652 - 16ms/epoch - 16ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.5096 - acc: 0.8706 - 15ms/epoch - 15ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.5047 - acc: 0.8752 - 16ms/epoch - 16ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.4998 - acc: 0.8800 - 16ms/epoch - 16ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.4951 - acc: 0.8846 - 15ms/epoch - 15ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.4904 - acc: 0.8887 - 16ms/epoch - 16ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.4857 - acc: 0.8927 - 17ms/epoch - 17ms/step\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.4811 - acc: 0.8975 - 16ms/epoch - 16ms/step\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.4766 - acc: 0.9018 - 16ms/epoch - 16ms/step\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.4722 - acc: 0.9047 - 17ms/epoch - 17ms/step\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.4678 - acc: 0.9084 - 16ms/epoch - 16ms/step\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.4635 - acc: 0.9116 - 20ms/epoch - 20ms/step\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.4593 - acc: 0.9148 - 26ms/epoch - 26ms/step\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.4551 - acc: 0.9176 - 15ms/epoch - 15ms/step\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.4509 - acc: 0.9198 - 23ms/epoch - 23ms/step\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.4468 - acc: 0.9227 - 19ms/epoch - 19ms/step\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.4428 - acc: 0.9250 - 18ms/epoch - 18ms/step\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.4389 - acc: 0.9277 - 16ms/epoch - 16ms/step\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.4349 - acc: 0.9304 - 16ms/epoch - 16ms/step\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.4311 - acc: 0.9334 - 15ms/epoch - 15ms/step\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.4273 - acc: 0.9355 - 16ms/epoch - 16ms/step\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.4235 - acc: 0.9373 - 16ms/epoch - 16ms/step\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.4198 - acc: 0.9387 - 15ms/epoch - 15ms/step\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.4162 - acc: 0.9399 - 17ms/epoch - 17ms/step\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.4126 - acc: 0.9412 - 17ms/epoch - 17ms/step\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.4090 - acc: 0.9422 - 16ms/epoch - 16ms/step\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.4055 - acc: 0.9439 - 18ms/epoch - 18ms/step\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.4021 - acc: 0.9453 - 17ms/epoch - 17ms/step\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.3986 - acc: 0.9464 - 17ms/epoch - 17ms/step\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.3953 - acc: 0.9474 - 15ms/epoch - 15ms/step\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.3920 - acc: 0.9491 - 17ms/epoch - 17ms/step\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.3887 - acc: 0.9508 - 16ms/epoch - 16ms/step\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.3854 - acc: 0.9527 - 15ms/epoch - 15ms/step\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.3822 - acc: 0.9540 - 16ms/epoch - 16ms/step\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.3791 - acc: 0.9554 - 17ms/epoch - 17ms/step\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.3760 - acc: 0.9572 - 16ms/epoch - 16ms/step\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.3729 - acc: 0.9580 - 17ms/epoch - 17ms/step\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.3699 - acc: 0.9593 - 15ms/epoch - 15ms/step\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.3669 - acc: 0.9598 - 16ms/epoch - 16ms/step\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.3640 - acc: 0.9604 - 16ms/epoch - 16ms/step\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.3611 - acc: 0.9620 - 16ms/epoch - 16ms/step\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.3582 - acc: 0.9623 - 17ms/epoch - 17ms/step\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.3554 - acc: 0.9630 - 15ms/epoch - 15ms/step\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.3526 - acc: 0.9638 - 16ms/epoch - 16ms/step\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.3498 - acc: 0.9644 - 17ms/epoch - 17ms/step\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.3471 - acc: 0.9648 - 16ms/epoch - 16ms/step\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.3444 - acc: 0.9653 - 16ms/epoch - 16ms/step\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.3417 - acc: 0.9661 - 15ms/epoch - 15ms/step\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.3391 - acc: 0.9669 - 14ms/epoch - 14ms/step\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.3365 - acc: 0.9673 - 15ms/epoch - 15ms/step\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.3340 - acc: 0.9673 - 14ms/epoch - 14ms/step\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.3314 - acc: 0.9676 - 15ms/epoch - 15ms/step\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.3289 - acc: 0.9681 - 14ms/epoch - 14ms/step\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.3265 - acc: 0.9686 - 15ms/epoch - 15ms/step\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.3241 - acc: 0.9690 - 16ms/epoch - 16ms/step\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.3217 - acc: 0.9693 - 16ms/epoch - 16ms/step\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.3193 - acc: 0.9694 - 15ms/epoch - 15ms/step\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.3170 - acc: 0.9695 - 16ms/epoch - 16ms/step\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.3146 - acc: 0.9698 - 14ms/epoch - 14ms/step\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.3124 - acc: 0.9700 - 17ms/epoch - 17ms/step\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.3101 - acc: 0.9704 - 16ms/epoch - 16ms/step\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.3079 - acc: 0.9706 - 14ms/epoch - 14ms/step\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.3057 - acc: 0.9707 - 15ms/epoch - 15ms/step\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.3035 - acc: 0.9711 - 14ms/epoch - 14ms/step\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.3014 - acc: 0.9712 - 15ms/epoch - 15ms/step\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.2993 - acc: 0.9714 - 14ms/epoch - 14ms/step\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.2972 - acc: 0.9716 - 14ms/epoch - 14ms/step\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.2951 - acc: 0.9718 - 13ms/epoch - 13ms/step\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.2931 - acc: 0.9718 - 26ms/epoch - 26ms/step\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.2911 - acc: 0.9721 - 15ms/epoch - 15ms/step\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.2891 - acc: 0.9721 - 14ms/epoch - 14ms/step\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.2871 - acc: 0.9723 - 16ms/epoch - 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a78a089a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = no_epochs, batch_size = len(train), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19000de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 559us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a593fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded = [int(round(x[0])) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee8ec6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 4969, 1: 31})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(rounded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
